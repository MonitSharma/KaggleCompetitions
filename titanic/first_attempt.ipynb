{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Step 1: Load and Prepare Data\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Add a column to differentiate train and test sets\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "data = pd.concat([train, test], sort=False)\n",
    "\n",
    "# Fill missing values\n",
    "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
    "data['Fare'] = data['Fare'].fillna(data['Fare'].median())\n",
    "\n",
    "# Feature engineering\n",
    "data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "data['Title'] = data['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "data['Title'] = data['Title'].replace({'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs'})\n",
    "data = pd.get_dummies(data, columns=['Title'], drop_first=True)\n",
    "data['Sex'] = LabelEncoder().fit_transform(data['Sex'])\n",
    "data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)\n",
    "data['FamilySize'] = data['SibSp'] + data['Parch']\n",
    "data['IsAlone'] = (data['FamilySize'] == 0).astype(int)\n",
    "data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'is_train'], axis=1, inplace=True)\n",
    "\n",
    "# Split data back into train and test\n",
    "train = data[data['Survived'].notna()]\n",
    "test = data[data['Survived'].isna()]\n",
    "X = train.drop('Survived', axis=1)\n",
    "y = train['Survived']\n",
    "X_test = test.drop('Survived', axis=1)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: submission_logistic.csv created\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_model = LogisticRegression(random_state=42)\n",
    "log_model.fit(X, y)\n",
    "log_preds = log_model.predict(X_test)\n",
    "\n",
    "# Save submission\n",
    "submission = pd.read_csv('dataset/gender_submission.csv')\n",
    "submission['Survived'] = log_preds\n",
    "submission.to_csv('dataset/submission_logistic.csv', index=False)\n",
    "print(\"Logistic Regression: submission_logistic.csv created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: submission_random_forest.csv created\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Save submission\n",
    "submission['Survived'] = rf_preds\n",
    "submission.to_csv('dataset/submission_random_forest.csv', index=False)\n",
    "print(\"Random Forest: submission_random_forest.csv created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: submission_xgboost.csv created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monitsharma/codes/Kaggle_competitions/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [12:44:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "xgb_preds = xgb_model.predict(X_test)\n",
    "\n",
    "# Save submission\n",
    "submission['Survived'] = xgb_preds\n",
    "submission.to_csv('dataset/submission_xgboost.csv', index=False)\n",
    "print(\"XGBoost: submission_xgboost.csv created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier: submission_bagging.csv created\n"
     ]
    }
   ],
   "source": [
    "# Bagging Classifier\n",
    "bagging_model = BaggingClassifier(estimator=rf_model, n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X, y)\n",
    "bagging_preds = bagging_model.predict(X_test)\n",
    "\n",
    "# Save submission\n",
    "submission['Survived'] = bagging_preds\n",
    "submission.to_csv('dataset/submission_bagging.csv', index=False)\n",
    "print(\"Bagging Classifier: submission_bagging.csv created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: submission_voting.csv created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monitsharma/codes/Kaggle_competitions/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [12:46:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', log_model),\n",
    "        ('rf', rf_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_model.fit(X, y)\n",
    "voting_preds = voting_model.predict(X_test)\n",
    "\n",
    "# Save submission\n",
    "submission['Survived'] = voting_preds\n",
    "submission.to_csv('dataset/submission_voting.csv', index=False)\n",
    "print(\"Voting Classifier: submission_voting.csv created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.3897\n",
      "Epoch [20/50], Loss: 0.3724\n",
      "Epoch [30/50], Loss: 0.3607\n",
      "Epoch [40/50], Loss: 0.3510\n",
      "Epoch [50/50], Loss: 0.3442\n",
      "Neural Network: submission_neural_network.csv created\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define Neural Network\n",
    "class TitanicNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TitanicNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "nn_model = TitanicNN(input_size=X_tensor.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    nn_model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nn_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Predict with Neural Network\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    nn_preds = nn_model(X_test_tensor).squeeze()\n",
    "    nn_preds = (nn_preds >= 0.5).int().numpy()\n",
    "\n",
    "# Save submission\n",
    "submission['Survived'] = nn_preds\n",
    "submission.to_csv('dataset/submission_neural_network.csv', index=False)\n",
    "print(\"Neural Network: submission_neural_network.csv created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE for Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Random Forest: submission_smote_rf.csv created\n"
     ]
    }
   ],
   "source": [
    "# Handle class imbalance with SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_smote, y_smote = sm.fit_resample(X, y)\n",
    "\n",
    "# Train Random Forest on SMOTE data\n",
    "rf_smote_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_smote_model.fit(X_smote, y_smote)\n",
    "rf_smote_preds = rf_smote_model.predict(X_test)\n",
    "\n",
    "# Save submission\n",
    "submission['Survived'] = rf_smote_preds\n",
    "submission.to_csv('dataset/submission_smote_rf.csv', index=False)\n",
    "print(\"SMOTE Random Forest: submission_smote_rf.csv created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
